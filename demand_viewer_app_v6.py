# demand_viewer_app_v6.py (v6_final11)
# ã‚³ãƒ¼ãƒ—ã•ã£ã½ã‚ï½œãƒ‡ãƒãƒ³ãƒ‰å€¤ãƒ“ãƒ¥ãƒ¼ã‚¢ï¼ˆå›ºå®šã‚¹ã‚­ãƒ¼ãƒãƒ»é«˜é€Ÿå¯¾å¿œãƒ»æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ï¼‰
# - æ¯”è¼ƒ: BãŒAã«å¯¾ã—ã¦ã©ã‚Œã ã‘å¤‰åŒ–?ï¼ˆå·®åˆ†ãƒ»å¢—æ¸›ç‡ï¼‰
# - æ™‚ç³»åˆ—A vs B: ç›¸å¯¾æ—¥ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼ˆé‡ã­æç”»ï¼‰
# - æ™‚åˆ»åˆ¥å½¢çŠ¶æ¯”è¼ƒã¯å¤–ã—ã€æ—¥ä»˜è»¸ã®æ¯”è¼ƒã‚’ä¸»è»¸
# - ã‚µã‚¤ãƒ‰ãƒãƒ¼æœŸé–“ã¨æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ã¯ç‹¬ç«‹
# - æ—¢çŸ¥ãƒã‚°ä¿®æ­£: normalize_date_series å†…ã®æ‹¬å¼§ã‚¿ã‚¤ãƒ—ãƒŸã‚¹

import streamlit as st
import pandas as pd
import numpy as np
import re, calendar
from datetime import date, datetime, time as dtime
from pathlib import Path

st.set_page_config(page_title="ãƒ‡ãƒãƒ³ãƒ‰å€¤ãƒ“ãƒ¥ãƒ¼ã‚¢ï¼ˆé«˜é€Ÿï¼‹æ¯”è¼ƒï¼‰", layout="wide")
st.title("ã‚³ãƒ¼ãƒ—ã•ã£ã½ã‚ï½œãƒ‡ãƒãƒ³ãƒ‰å€¤ãƒ“ãƒ¥ãƒ¼ã‚¢ï¼ˆé«˜é€Ÿï¼‹æ¯”è¼ƒï¼‰")
st.caption("Parqueté«˜é€Ÿãƒ»Excelæœ€é©åŒ–ï¼åˆç®—ãƒ„ãƒ¼ãƒ«ï¼A-Bæ¯”è¼ƒãƒ„ãƒ¼ãƒ«ï¼ˆæ¯”è¼ƒã¯å…¨æœŸé–“ã§ç‹¬ç«‹ï¼‰")

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def month_end(y: int, m: int) -> date:
    return date(y, m, calendar.monthrange(y, m)[1])

def parse_year_month_from_sheet(sheet_name: str):
    m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ", str(sheet_name))
    if not m:
        return None
    return int(m.group(1)), int(m.group(2))

def sheet_overlaps_period(y: int, m: int, start_date: date, end_date: date) -> bool:
    s = date(y, m, 1)
    e = month_end(y, m)
    return not (e < start_date or s > end_date)

def normalize_date_series(s: pd.Series) -> pd.Series:
    """
    YYYYMMDD / Excel Serial / æ˜ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(%Y-%m-%d, %Y/%m/%d, %Y.%m.%d) ã‚’è¨±å®¹ã€‚
    ã™ã¹ã¦ datetime64[ns] æ­£è¦åŒ–ï¼ˆ00:00å›ºå®šï¼‰ã¾ã§è¡Œã†ã€‚
    """
    s_str = s.astype(str).str.strip()

    # 1) YYYYMMDDï¼ˆ8æ¡ï¼‰
    mask_ymd = s_str.str.fullmatch(r"\d{8}")
    dt1 = pd.to_datetime(s_str.where(mask_ymd, np.nan), format="%Y%m%d", errors="coerce")

    # 2) Excel Serialï¼ˆæ•°å€¤ï¼‰
    s_num = pd.to_numeric(s_str.where(~mask_ymd, np.nan), errors="coerce")
    mask_serial = s_num.notna() & (s_num > 0)
    dt2 = pd.Series(pd.NaT, index=s.index, dtype="datetime64[ns]")
    if mask_serial.any():
        dt2.loc[mask_serial] = pd.to_datetime(
            s_num.loc[mask_serial], unit="D", origin="1899-12-30", errors="coerce"
        )

    # 3) æ˜ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    leftover_idx = s.index[dt1.isna() & dt2.isna()]
    dt3 = pd.Series(pd.NaT, index=s.index, dtype="datetime64[ns]")
    if len(leftover_idx) > 0:
        rem = s_str.loc[leftover_idx]
        tried = pd.Series(pd.NaT, index=leftover_idx, dtype="datetime64[ns]")  # â† ä¿®æ­£: æ‹¬å¼§
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"):
            mask_try = tried.isna()
            if mask_try.any():
                cand = pd.to_datetime(rem[mask_try], format=fmt, errors="coerce")
                tried.loc[mask_try] = cand
        dt3 = tried

    dt = dt1.fillna(dt2).fillna(dt3)
    return pd.to_datetime(dt, errors="coerce").dt.normalize()

def to_long_after_prefilter(df: pd.DataFrame, time_cols: list[str]) -> pd.DataFrame:
    long = df.melt(
        id_vars=["åº—èˆ—å","æ—¥ä»˜"],
        value_vars=time_cols,
        var_name="æ™‚é–“å¸¯",
        value_name="ãƒ‡ãƒãƒ³ãƒ‰å€¤"
    ).dropna(subset=["åº—èˆ—å","æ—¥ä»˜","ãƒ‡ãƒãƒ³ãƒ‰å€¤"], how="any")

    long["æ™‚é–“å¸¯"] = long["æ™‚é–“å¸¯"].astype(str).str.replace("\u3000", " ").str.strip()
    long["é–‹å§‹æ™‚åˆ»"] = np.where(
        long["æ™‚é–“å¸¯"].str.contains("-"),
        long["æ™‚é–“å¸¯"].str.split("-").str[0].str.strip(),
        long["æ™‚é–“å¸¯"]
    )
    long["æ—¥æ™‚"] = pd.to_datetime(long["æ—¥ä»˜"].dt.date.astype(str) + " " + long["é–‹å§‹æ™‚åˆ»"], errors="coerce")
    return long

def type_optimize_long(df: pd.DataFrame) -> pd.DataFrame:
    if "åº—èˆ—å" in df.columns:
        df["åº—èˆ—å"] = df["åº—èˆ—å"].astype("category")
    for c in ["æ™‚é–“å¸¯","é–‹å§‹æ™‚åˆ»"]:
        if c in df.columns:
            df[c] = df[c].astype("category")
    if "ãƒ‡ãƒãƒ³ãƒ‰å€¤" in df.columns:
        df["ãƒ‡ãƒãƒ³ãƒ‰å€¤"] = pd.to_numeric(df["ãƒ‡ãƒãƒ³ãƒ‰å€¤"], errors="coerce").astype("float32")
    if "æ—¥æ™‚" in df.columns:
        df["æ—¥æ™‚"] = pd.to_datetime(df["æ—¥æ™‚"], errors="coerce")
    if "æ—¥ä»˜" in df.columns:
        df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
    return df

# ===== Excel èª­ã¿è¾¼ã¿ï¼ˆæœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼‰=====
@st.cache_data(show_spinner=True, ttl=1800, max_entries=8)
def load_excel_filtered(path_or_file, start_date: date, end_date: date, early_store_filter: list[str] | None):
    xls = pd.ExcelFile(path_or_file)
    frames = []
    diags = []
    start_ts, end_ts = pd.Timestamp(start_date), pd.Timestamp(end_date)

    for sheet in xls.sheet_names:
        ym = parse_year_month_from_sheet(sheet)
        if ym:
            y, m = ym
            if not sheet_overlaps_period(y, m, start_date, end_date):
                continue

        df = pd.read_excel(xls, sheet_name=sheet, header=0)
        if df.shape[1] < 3:
            continue

        cols = list(df.columns)
        store_col, date_col = cols[0], cols[1]
        time_cols = [c for c in cols[2:] if df[c].notna().any()]
        if not time_cols:
            continue

        df = df.rename(columns={store_col: "åº—èˆ—å", date_col: "æ—¥ä»˜"})
        df["æ—¥ä»˜"] = normalize_date_series(df["æ—¥ä»˜"])

        # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
        mdate = (df["æ—¥ä»˜"] >= start_ts) & (df["æ—¥ä»˜"] <= end_ts)
        df = df.loc[mdate]

        # åº—èˆ—ãƒ•ã‚£ãƒ«ã‚¿
        if early_store_filter:
            df = df[df["åº—èˆ—å"].isin(early_store_filter)]

        if df.empty:
            continue

        df[time_cols] = df[time_cols].apply(pd.to_numeric, errors="coerce").astype("float32")

        long = to_long_after_prefilter(df[["åº—èˆ—å","æ—¥ä»˜"] + time_cols], time_cols)
        long = type_optimize_long(long)
        frames.append(long)

        diags.append({
            "sheet": sheet,
            "first_cols": cols[:8],
            "n_time_cols_used": len(time_cols),
            "n_rows": int(len(long)),
        })

    if not frames:
        return pd.DataFrame(columns=["åº—èˆ—å","æ—¥ä»˜","æ™‚é–“å¸¯","ãƒ‡ãƒãƒ³ãƒ‰å€¤","é–‹å§‹æ™‚åˆ»","æ—¥æ™‚"]), diags

    data = pd.concat(frames, ignore_index=True)
    data = data.dropna(subset=["åº—èˆ—å","æ—¥ä»˜","ãƒ‡ãƒãƒ³ãƒ‰å€¤"])
    return data, diags

# ===== Parquet é«˜é€Ÿ =====
def parquet_dir_path(base_dir: Path) -> Path:
    return base_dir / "data_parquet"

def parquet_available(parquet_dir: Path) -> bool:
    return parquet_dir.exists() and any(parquet_dir.glob("*.parquet"))

@st.cache_data(show_spinner=True, ttl=3600, max_entries=16)
def load_parquet_fast(parquet_dir: Path, start_date: date, end_date: date, early_store_filter: list[str] | None):
    files = sorted(parquet_dir.glob("*.parquet"))
    if not files:
        return pd.DataFrame(columns=["åº—èˆ—å","æ—¥ä»˜","æ™‚é–“å¸¯","ãƒ‡ãƒãƒ³ãƒ‰å€¤","é–‹å§‹æ™‚åˆ»","æ—¥æ™‚"])

    start_ts, end_ts = pd.Timestamp(start_date), pd.Timestamp(end_date)
    pick = []
    for f in files:
        m = re.search(r"(\d{4})-(\d{2})\.parquet", f.name)
        if m:
            y, mm = int(m.group(1)), int(m.group(2))
            if sheet_overlaps_period(y, mm, start_date, end_date):
                pick.append(f)
        else:
            pick.append(f)

    dfs = []
    for f in pick:
        df = pd.read_parquet(f)
        dcol = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
        m1 = (dcol >= start_ts) & (dcol <= end_ts)
        if early_store_filter:
            m2 = df["åº—èˆ—å"].isin(early_store_filter)
            df = df.loc[m1 & m2]
        else:
            df = df.loc[m1]
        dfs.append(df)

    if not dfs:
        return pd.DataFrame(columns=["åº—èˆ—å","æ—¥ä»˜","æ™‚é–“å¸¯","ãƒ‡ãƒãƒ³ãƒ‰å€¤","é–‹å§‹æ™‚åˆ»","æ—¥æ™‚"])

    out = pd.concat(dfs, ignore_index=True)
    out = type_optimize_long(out)
    return out

# ===== Parquet å‰å‡¦ç†ï¼ˆExcelâ†’Parquetï¼‰=====
@st.cache_data(show_spinner=True, ttl=0)
def preconvert_to_parquet(path_or_file, outdir: Path):
    xls = pd.ExcelFile(path_or_file)
    outdir.mkdir(exist_ok=True)
    results = []

    for sheet in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name=sheet, header=0)
        if df.shape[1] < 3:
            continue
        cols = list(df.columns)
        df = df.rename(columns={cols[0]: "åº—èˆ—å", cols[1]: "æ—¥ä»˜"})
        time_cols = [c for c in cols[2:] if df[c].notna().any()]
        if not time_cols:
            continue

        df["æ—¥ä»˜"] = normalize_date_series(df["æ—¥ä»˜"])
        df[time_cols] = df[time_cols].apply(pd.to_numeric, errors="coerce").astype("float32")
        df["åº—èˆ—å"] = df["åº—èˆ—å"].astype("category")

        long = to_long_after_prefilter(df[["åº—èˆ—å","æ—¥ä»˜"] + time_cols], time_cols)
        long = type_optimize_long(long)

        m = parse_year_month_from_sheet(sheet)
        if m:
            y, mm = m
            fname = f"{y}-{mm:02d}.parquet"
        else:
            safe = re.sub(r"[^\w\-]+", "_", str(sheet))
            fname = f"{safe}.parquet"

        long.to_parquet(outdir / fname, index=False)
        results.append({"sheet": sheet, "rows": int(len(long)), "file": fname})

    return results

# ===== æœŸé–“ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å®š =====
def guess_period_from_sources():
    min_guess = date(2000,1,1)
    max_guess = date(2030,12,31)
    if parquet_available(pq_dir):
        files = sorted(pq_dir.glob("*.parquet"))
        yms = []
        for f in files:
            m = re.search(r"(\d{4})-(\d{2})\.parquet", f.name)
            if m:
                yms.append((int(m.group(1)), int(m.group(2))))
        if yms:
            y0, m0 = yms[0]; y1, m1 = yms[-1]
            return date(y0, m0, 1), month_end(y1, m1)
    elif file is not None:
        try:
            xls_tmp = pd.ExcelFile(file)
            yms = [parse_year_month_from_sheet(sh) for sh in xls_tmp.sheet_names]
            yms = [t for t in yms if t]
            if yms:
                y0, m0 = yms[0]; y1, m1 = yms[-1]
                return date(y0, m0, 1), month_end(y1, m1)
        except Exception:
            pass
    return min_guess, max_guess

def guess_full_period_for_compare():
    return guess_period_from_sources()

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ =====
st.sidebar.header("â‘  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå›ºå®šã‚¹ã‚­ãƒ¼ãƒï¼‰")
file = st.sidebar.file_uploader("Excelï¼ˆ.xlsxï¼‰ã‚’é¸æŠï¼ˆA=åº—èˆ—å, B=æ—¥ä»˜, C..=æ™‚é–“å¸¯ï¼‰", type=["xlsx"])

base_dir = Path.cwd()
pq_dir = parquet_dir_path(base_dir)

st.sidebar.header("é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰")
use_fast = st.sidebar.toggle("Parquetï¼ˆå‰å‡¦ç†æ¸ˆã¿ï¼‰ã‚’å„ªå…ˆã—ã¦ä½¿ã†", value=True)

if file is not None:
    if st.sidebar.button("âš¡ å‰å‡¦ç†ã—ã¦Parquetç”Ÿæˆï¼ˆé«˜é€ŸåŒ–Aï¼‰"):
        with st.spinner("å‰å‡¦ç†ä¸­ï¼ˆExcelâ†’æœˆåˆ¥Parquetï¼‰..."):
            try:
                res = preconvert_to_parquet(file, pq_dir)
                st.sidebar.success(f"Parquetä½œæˆ: {len(res)} ãƒ•ã‚¡ã‚¤ãƒ«")
            except Exception as e:
                st.sidebar.error("å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼")
                st.sidebar.exception(e)

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ æœŸé–“ =====
min_guess, max_guess = guess_period_from_sources()

st.sidebar.header("â‘¡ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
if st.sidebar.button("ğŸ”„ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆæœŸåŒ–ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰"):
    st.cache_data.clear()
    st.session_state.clear()
    st.rerun()

date_range = st.sidebar.date_input(
    "å¯¾è±¡æœŸé–“ï¼ˆé–‹å§‹æ—¥ã€œçµ‚äº†æ—¥ï¼‰",
    value=(min_guess, max_guess),
    min_value=min_guess,
    max_value=max_guess,
    key="target_period_v6_final11"
)
if isinstance(date_range, tuple) and len(date_range) == 2:
    start_date, end_date = date_range
else:
    start_date, end_date = min_guess, max_guess

# åº—èˆ—å€™è£œã®æ¨å®š
store_candidates = []
try:
    if use_fast and parquet_available(pq_dir):
        tmp = load_parquet_fast(pq_dir, start_date, end_date, early_store_filter=None)
        store_candidates = (
            pd.Series(tmp["åº—èˆ—å"].dropna().astype(str).unique())
            .loc[lambda s: s.ne("") & ~s.str.fullmatch(r"\d+(\.\d+)?")]
            .sort_values()
            .tolist()
        )
    elif file is not None:
        xls = pd.ExcelFile(file)
        names = []
        for sh in xls.sheet_names:
            ym = parse_year_month_from_sheet(sh)
            if ym and not sheet_overlaps_period(ym[0], ym[1], start_date, end_date):
                continue
            small = pd.read_excel(xls, sheet_name=sh, usecols=[0,1], header=0)
            if small.shape[1] >= 2:
                dd = small.copy()
                dd.columns = ["åº—èˆ—å","æ—¥ä»˜"]
                dd["æ—¥ä»˜"] = normalize_date_series(dd["æ—¥ä»˜"])
                dd = dd.dropna(subset=["æ—¥ä»˜"])
                vals = (
                    dd["åº—èˆ—å"].dropna().astype(str).str.strip()
                    .loc[lambda s: s.ne("") & ~s.str.fullmatch(r"\d+(\.\d+)?")]
                    .unique().tolist()
                )
                names.extend(vals)
        store_candidates = sorted(set(names))
except Exception:
    pass

selected_stores = st.sidebar.multiselect(
    "åº—èˆ—åï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
    store_candidates,
    default=(store_candidates[:1] if store_candidates else [])
)

time_slots_all = [f"{h:02d}:{m:02d}" for h in range(24) for m in (0,30)]
start_time = st.sidebar.selectbox("é–‹å§‹æ™‚åˆ»ï¼ˆè¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿ï¼‰", time_slots_all, index=0)
end_time   = st.sidebar.selectbox("çµ‚äº†æ™‚åˆ»ï¼ˆè¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿ãƒ»å«ã‚€ï¼‰", time_slots_all, index=len(time_slots_all)-1)

agg_level = st.sidebar.selectbox("é›†è¨ˆç²’åº¦", ["30åˆ†ï¼ˆãã®ã¾ã¾ï¼‰", "æ™‚é–“åˆ¥ï¼ˆåˆè¨ˆï¼‰", "æ—¥åˆ¥ï¼ˆåˆè¨ˆï¼‰"])

# åˆç®—ãƒˆã‚°ãƒ«
if "show_sum" not in st.session_state:
    st.session_state["show_sum"] = False
c1, c2 = st.sidebar.columns(2)
if c1.button("ğŸ”¢ åˆç®—ã‚’è¡¨ç¤º/æ›´æ–°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å…¨ä½“ï¼‰"):
    st.session_state["show_sum"] = True
if c2.button("ğŸ™ˆ åˆç®—ã‚’éš ã™"):
    st.session_state["show_sum"] = False

# æ¯”è¼ƒãƒˆã‚°ãƒ«ï¼ˆç‹¬ç«‹ï¼‰
if "show_compare" not in st.session_state:
    st.session_state["show_compare"] = False
cc1, cc2 = st.sidebar.columns(2)
if cc1.button("ğŸ” æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ã‚’é–‹ã"):
    st.session_state["show_compare"] = True
if cc2.button("âœ– æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ã‚’é–‰ã˜ã‚‹"):
    st.session_state["show_compare"] = False

# ===== ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼æœŸé–“ï¼‰=====
if use_fast and parquet_available(pq_dir):
    with st.spinner("Parqueté«˜é€Ÿèª­è¾¼ä¸­..."):
        data = load_parquet_fast(pq_dir, start_date, end_date, early_store_filter=selected_stores or None)
        diags = []
else:
    if file is None:
        st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€Parqueté«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")
        st.stop()
    with st.spinner("Excel èª­è¾¼ãƒ»æœ€é©åŒ–ä¸­..."):
        data, diags = load_excel_filtered(file, start_date, end_date, early_store_filter=selected_stores or None)

with st.expander("èª­ã¿è¾¼ã¿è¨ºæ–­æƒ…å ±ï¼ˆå…ˆé ­10ä»¶ï¼‰"):
    st.write(diags[:10] if diags else "Parqueté«˜é€Ÿãƒ¢ãƒ¼ãƒ‰")

if data.empty:
    st.warning("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

# æ™‚åˆ»ãƒ•ã‚£ãƒ«ã‚¿
data["é–‹å§‹æ™‚åˆ»_only"] = pd.to_datetime(data["æ—¥æ™‚"], errors="coerce").dt.strftime("%H:%M")
mask_time = (data["é–‹å§‹æ™‚åˆ»_only"] >= start_time) & (data["é–‹å§‹æ™‚åˆ»_only"] <= end_time)
filtered = data.loc[mask_time].copy()
if filtered.empty:
    st.warning("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

# å¯è¦–åŒ–ç”¨é›†è¨ˆ
if agg_level == "30åˆ†ï¼ˆãã®ã¾ã¾ï¼‰":
    show = filtered.sort_values(["åº—èˆ—å","æ—¥æ™‚"])
    xcol = "æ—¥æ™‚"
elif agg_level == "æ™‚é–“åˆ¥ï¼ˆåˆè¨ˆï¼‰":
    filtered["hour"] = pd.to_datetime(filtered["æ—¥æ™‚"], errors="coerce").dt.floor("H")
    show = (
        filtered.groupby(["åº—èˆ—å","hour"], as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"]
        .sum().rename(columns={"hour":"æ—¥æ™‚"})
    )
    xcol = "æ—¥æ™‚"
else:
    filtered["æ—¥ä»˜_d"] = pd.to_datetime(filtered["æ—¥ä»˜"], errors="coerce").dt.date
    show = (
        filtered.groupby(["åº—èˆ—å","æ—¥ä»˜_d"], as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"]
        .sum().rename(columns={"æ—¥ä»˜_d":"æ—¥ä»˜"})
    )
    xcol = "æ—¥ä»˜"

# é‡è¤‡åˆ—æŠ‘æ­¢
cols_drop = []
if "æ—¥ä»˜" in show.columns and "æ—¥æ™‚" in show.columns:
    if "æ—¥æ™‚" == xcol:
        cols_drop.append("æ—¥ä»˜")
    else:
        cols_drop.append("æ—¥æ™‚")
for c in ["é–‹å§‹æ™‚åˆ»_only"]:
    if c in show.columns: cols_drop.append(c)
if cols_drop:
    show = show.drop(columns=cols_drop)

# ===== åˆç®—ã‚»ã‚¯ã‚·ãƒ§ãƒ³ =====
st.subheader("é›†è¨ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåˆè¨ˆãƒ„ãƒ¼ãƒ«ï¼‰")
if not st.session_state.get("show_sum", False):
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã® **ã€ŒğŸ”¢ åˆç®—ã‚’è¡¨ç¤º/æ›´æ–°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å…¨ä½“ï¼‰ã€** ã‚’æŠ¼ã™ã¨å±•é–‹ã•ã‚Œã¾ã™ã€‚")
else:
    total_sum = float(filtered["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum())
    k1, k2, k3 = st.columns(3)
    k1.metric("ç·åˆè¨ˆï¼ˆè¡¨ç¤ºä¸­ã®æ™‚é–“å¸¯æ¡ä»¶ï¼‰", f"{total_sum:,.2f}")
    by_store = (
        filtered.groupby("åº—èˆ—å", as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"]
        .sum().sort_values("ãƒ‡ãƒãƒ³ãƒ‰å€¤", ascending=False)
    )
    k2.metric("åº—èˆ—æ•°ï¼ˆé¸æŠï¼‰", f"{by_store['åº—èˆ—å'].nunique():,}")
    k3.metric("ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼ˆé¸æŠï¼‰", f"{len(filtered):,}")

    with st.expander("åº—èˆ—åˆ¥ åˆè¨ˆï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯ï¼‰", expanded=True):
        st.dataframe(by_store, use_container_width=True)
        st.download_button(
            "åº—èˆ—åˆ¥_åˆè¨ˆ.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=by_store.to_csv(index=False).encode("utf-8-sig"),
            file_name="åº—èˆ—åˆ¥_åˆè¨ˆ.csv",
            mime="text/csv",
        )

    # é¸æŠè¡Œåˆç®—ï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰
    st.markdown("### ğŸ§® é¸æŠè¡Œã®åˆç®—ï¼ˆè¡¨ã‹ã‚‰ãƒã‚§ãƒƒã‚¯ï¼‰")
    if "table_rev_v6" not in st.session_state:
        st.session_state["table_rev_v6"] = 0
    editor_key = f"table_editor_v6_final11_{st.session_state['table_rev_v6']}"
    table_df = show.reset_index(drop=True).copy()
    if "é¸æŠ" not in table_df.columns:
        table_df.insert(0, "é¸æŠ", False)
    disabled_cols = [c for c in table_df.columns if c != "é¸æŠ"]

    edited = st.data_editor(
        table_df,
        use_container_width=True,
        num_rows="fixed",
        column_config={"é¸æŠ": st.column_config.CheckboxColumn("é¸æŠ", help="åˆç®—ã—ãŸã„è¡Œã«ãƒã‚§ãƒƒã‚¯")},
        disabled=disabled_cols,
        key=editor_key,
    )

    cc1, cc2 = st.columns([1,1])
    if "show_sum_selected" not in st.session_state:
        st.session_state["show_sum_selected"] = False
    if cc1.button("ğŸ§® åˆç®—ï¼ˆé¸æŠè¡Œï¼‰ã‚’è¡¨ç¤º/æ›´æ–°"):
        st.session_state["show_sum_selected"] = True
    if cc2.button("ğŸ§¹ é¸æŠã‚’ã‚¯ãƒªã‚¢"):
        st.session_state["table_rev_v6"] += 1
        st.session_state["show_sum_selected"] = False
        st.rerun()

    if st.session_state.get("show_sum_selected", False):
        sel = edited[edited["é¸æŠ"] == True].copy()
        if sel.empty:
            st.info("ãƒã‚§ãƒƒã‚¯ã•ã‚ŒãŸè¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            total_sel = float(sel["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum())
            s1, s2 = st.columns(2)
            s1.metric("é¸æŠè¡Œã®åˆè¨ˆ", f"{total_sel:,.2f}")
            if "åº—èˆ—å" in sel.columns:
                by_store_sel = sel.groupby("åº—èˆ—å", as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum().sort_values("ãƒ‡ãƒãƒ³ãƒ‰å€¤", ascending=False)
                with st.expander("é¸æŠè¡Œï¼šåº—èˆ—åˆ¥ åˆè¨ˆ", expanded=True):
                    st.dataframe(by_store_sel, use_container_width=True)

    # ä»»æ„æœŸé–“ åˆç®—ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼æ™‚é–“å¸¯ç„¡è¦–ï¼‰
    st.markdown("### â± ä»»æ„ã®é–‹å§‹æ—¥æ™‚ã€œçµ‚äº†æ—¥æ™‚ã§åˆç®—ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼æ™‚é–“å¸¯ã¯ç„¡è¦–ï¼‰")
    min_dt = datetime.combine(start_date, dtime(0,0))
    max_dt = datetime.combine(end_date, dtime(23,30))
    col_a, col_b = st.columns(2)
    dt_start = col_a.date_input("é–‹å§‹æ—¥ï¼ˆåˆç®—ç”¨ï¼‰", value=min_dt.date(), min_value=start_date, max_value=end_date, key="sum_custom_start_date")
    tm_start = col_a.time_input("é–‹å§‹æ™‚åˆ»ï¼ˆåˆç®—ç”¨ï¼‰", value=dtime(0,0), step=1800, key="sum_custom_start_time")
    dt_end   = col_b.date_input("çµ‚äº†æ—¥ï¼ˆåˆç®—ç”¨ï¼‰", value=max_dt.date(), min_value=start_date, max_value=end_date, key="sum_custom_end_date")
    tm_end   = col_b.time_input("çµ‚äº†æ™‚åˆ»ï¼ˆåˆç®—ç”¨ï¼‰", value=dtime(23,30), step=1800, key="sum_custom_end_time")

    if st.button("â± ã“ã®é–‹å§‹æ—¥æ™‚ã€œçµ‚äº†æ—¥æ™‚ã§åˆç®—ã™ã‚‹"):
        try:
            start_dt = datetime.combine(dt_start, tm_start)
            end_dt   = datetime.combine(dt_end, tm_end)
            if start_dt > end_dt:
                st.error("é–‹å§‹æ—¥æ™‚ã¯çµ‚äº†æ—¥æ™‚ä»¥å‰ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            else:
                base = data.copy()
                base["æ—¥æ™‚"] = pd.to_datetime(base["æ—¥æ™‚"], errors="coerce")
                m = (base["æ—¥æ™‚"] >= pd.Timestamp(start_dt)) & (base["æ—¥æ™‚"] <= pd.Timestamp(end_dt))
                base = base.loc[m]
                if base.empty:
                    st.info("æŒ‡å®šç¯„å›²ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    total_custom = float(base["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum())
                    c1, c2 = st.columns(2)
                    c1.metric("ä»»æ„æœŸé–“ã®åˆè¨ˆ", f"{total_custom:,.2f}")
                    by_store_custom = base.groupby("åº—èˆ—å", as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum().sort_values("ãƒ‡ãƒãƒ³ãƒ‰å€¤", ascending=False)
                    with st.expander("ä»»æ„æœŸé–“ï¼šåº—èˆ—åˆ¥ åˆè¨ˆï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯ï¼‰", expanded=True):
                        st.dataframe(by_store_custom, use_container_width=True)
                        st.download_button(
                            "ä»»æ„æœŸé–“_åº—èˆ—åˆ¥_åˆè¨ˆ.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=by_store_custom.to_csv(index=False).encode("utf-8-sig"),
                            file_name="ä»»æ„æœŸé–“_åº—èˆ—åˆ¥_åˆè¨ˆ.csv",
                            mime="text/csv",
                        )
        except Exception as e:
            st.error("ä»»æ„æœŸé–“åˆç®—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            st.exception(e)

# ===== æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ï¼ˆå…¨æœŸé–“ãƒ»ç‹¬ç«‹ï¼‰=====
if st.session_state.get("show_compare", False):
    st.subheader("ğŸ” æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ï¼ˆåº—èˆ—1ã¤ãƒ»AæœŸé–“ vs BæœŸé–“ï½œå…¨æœŸé–“ç‹¬ç«‹ï¼‰")

    min_all, max_all = guess_full_period_for_compare()

    @st.cache_data(show_spinner=True, ttl=1800)
    def load_compare_data(file, pq_dir, use_fast, min_all, max_all):
        if use_fast and parquet_available(pq_dir):
            return load_parquet_fast(pq_dir, min_all, max_all, early_store_filter=None)
        else:
            if file is None:
                return pd.DataFrame(columns=["åº—èˆ—å","æ—¥ä»˜","æ™‚é–“å¸¯","ãƒ‡ãƒãƒ³ãƒ‰å€¤","é–‹å§‹æ™‚åˆ»","æ—¥æ™‚"])
            data_all, _ = load_excel_filtered(file, min_all, max_all, early_store_filter=None)
            return data_all

    with st.spinner("æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨æœŸé–“ï¼‰ã‚’æº–å‚™ä¸­..."):
        cmp_data = load_compare_data(file, pq_dir, use_fast, min_all, max_all)

    if cmp_data.empty:
        st.info("æ¯”è¼ƒå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå…¨æœŸé–“ï¼‰ã€‚")
    else:
        compare_stores = (
            pd.Series(cmp_data["åº—èˆ—å"].dropna().astype(str).unique())
            .loc[lambda s: s.ne("") & ~s.str.fullmatch(r"\d+(\.\d+)?")]
            .sort_values().tolist()
        )
        csel = st.selectbox("åº—èˆ—ï¼ˆæ¯”è¼ƒå¯¾è±¡ï¼‰", compare_stores, index=0, key="compare_store_v6_11")

        base_min_date = pd.to_datetime(cmp_data["æ—¥ä»˜"], errors="coerce").min().date()
        base_max_date = pd.to_datetime(cmp_data["æ—¥ä»˜"], errors="coerce").max().date()
        time_all = [f"{h:02d}:{m:02d}" for h in range(24) for m in (0,30)]

        st.markdown("#### æœŸé–“A")
        ca1, ca2 = st.columns([1,1])
        a_start_d = ca1.date_input("A é–‹å§‹æ—¥", value=base_min_date, min_value=base_min_date, max_value=base_max_date, key="cmp_A_sd_11")
        a_end_d   = ca1.date_input("A çµ‚äº†æ—¥", value=base_max_date, min_value=base_min_date, max_value=base_max_date, key="cmp_A_ed_11")
        a_start_t = ca2.selectbox("A é–‹å§‹æ™‚åˆ»", time_all, index=0, key="cmp_A_st_11")
        a_end_t   = ca2.selectbox("A çµ‚äº†æ™‚åˆ»ï¼ˆå«ã‚€ï¼‰", time_all, index=len(time_all)-1, key="cmp_A_et_11")

        st.markdown("#### æœŸé–“B")
        cb1, cb2 = st.columns([1,1])
        b_start_d = cb1.date_input("B é–‹å§‹æ—¥", value=base_min_date, min_value=base_min_date, max_value=base_max_date, key="cmp_B_sd_11")
        b_end_d   = cb1.date_input("B çµ‚äº†æ—¥", value=base_max_date, min_value=base_min_date, max_value=base_max_date, key="cmp_B_ed_11")
        b_start_t = cb2.selectbox("B é–‹å§‹æ™‚åˆ»", time_all, index=0, key="cmp_B_st_11")
        b_end_t   = cb2.selectbox("B çµ‚äº†æ™‚åˆ»ï¼ˆå«ã‚€ï¼‰", time_all, index=len(time_all)-1, key="cmp_B_et_11")

        run_compare = st.button("ğŸ†š ã“ã®è¨­å®šã§æ¯”è¼ƒã™ã‚‹ï¼ˆBãŒAã‹ã‚‰ã©ã‚Œã ã‘å¤‰åŒ–ï¼Ÿï¼‰")
        if run_compare:
            try:
                a_start_dt = pd.Timestamp(f"{a_start_d} {a_start_t}:00")
                a_end_dt   = pd.Timestamp(f"{a_end_d} {a_end_t}:00")
                b_start_dt = pd.Timestamp(f"{b_start_d} {b_start_t}:00")
                b_end_dt   = pd.Timestamp(f"{b_end_d} {b_end_t}:00")

                if a_start_dt > a_end_dt or b_start_dt > b_end_dt:
                    st.error("é–‹å§‹æ—¥æ™‚ã¯çµ‚äº†æ—¥æ™‚ä»¥å‰ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
                else:
                    base = cmp_data.copy()
                    base["æ—¥æ™‚"] = pd.to_datetime(base["æ—¥æ™‚"], errors="coerce")
                    base = base[base["åº—èˆ—å"].astype(str) == str(csel)]

                    A = base[(base["æ—¥æ™‚"] >= a_start_dt) & (base["æ—¥æ™‚"] <= a_end_dt)].copy()
                    B = base[(base["æ—¥æ™‚"] >= b_start_dt) & (base["æ—¥æ™‚"] <= b_end_dt)].copy()

                    if A.empty or B.empty:
                        st.info("Aã¾ãŸã¯Bã®æœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        # å…¨ä½“åˆè¨ˆã®æ¯”è¼ƒï¼ˆBãŒAã‹ã‚‰ã©ã‚Œã ã‘å¤‰åŒ–ã—ãŸã‹ï¼‰
                        A_sum = float(A["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum())
                        B_sum = float(B["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum())
                        diff  = B_sum - A_sum
                        pct   = (diff / A_sum * 100.0) if A_sum != 0 else np.nan

                        m1, m2, m3, m4 = st.columns(4)
                        m1.metric("A åˆè¨ˆ", f"{A_sum:,.2f}")
                        m2.metric("B åˆè¨ˆ", f"{B_sum:,.2f}")
                        m3.metric("å·®åˆ†(Bâˆ’A)", f"{diff:,.2f}")
                        m4.metric("å¢—æ¸›ç‡(%)", f"{pct:,.2f}" if np.isfinite(pct) else "N/A")

                        # ===== æ—¥åˆ¥åˆè¨ˆã§æ¯”è¼ƒï¼ˆåŸºæœ¬ï¼‰
                        A["æ—¥ä»˜_only"] = pd.to_datetime(A["æ—¥æ™‚"]).dt.date
                        B["æ—¥ä»˜_only"] = pd.to_datetime(B["æ—¥æ™‚"]).dt.date
                        Ag = A.groupby("æ—¥ä»˜_only", as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum().rename(columns={"ãƒ‡ãƒãƒ³ãƒ‰å€¤":"A_åˆè¨ˆ"})
                        Bg = B.groupby("æ—¥ä»˜_only", as_index=False)["ãƒ‡ãƒãƒ³ãƒ‰å€¤"].sum().rename(columns={"ãƒ‡ãƒãƒ³ãƒ‰å€¤":"B_åˆè¨ˆ"})

                        # ç›¸å¯¾æ—¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ1..Nï¼‰ã§æ•´åˆ— â†’ é‡ã­æç”»
                        Ag = Ag.sort_values("æ—¥ä»˜_only").reset_index(drop=True).assign(ç›¸å¯¾æ—¥=lambda df: df.index+1)
                        Bg = Bg.sort_values("æ—¥ä»˜_only").reset_index(drop=True).assign(ç›¸å¯¾æ—¥=lambda df: df.index+1)

                        # ç›¸å¯¾æ—¥ã§ãƒãƒ¼ã‚¸ã—ã¦æ—¥æ¯ã®å¤‰åŒ–é‡
                        comp = pd.merge(Ag, Bg, on="ç›¸å¯¾æ—¥", how="outer")
                        comp["A_åˆè¨ˆ"] = comp["A_åˆè¨ˆ"].fillna(0.0)
                        comp["B_åˆè¨ˆ"] = comp["B_åˆè¨ˆ"].fillna(0.0)
                        comp["å·®åˆ†(Bâˆ’A)"] = comp["B_åˆè¨ˆ"] - comp["A_åˆè¨ˆ"]
                        comp["å¢—æ¸›ç‡(%)"] = np.where(
                            comp["A_åˆè¨ˆ"] != 0, (comp["å·®åˆ†(Bâˆ’A)"]/comp["A_åˆè¨ˆ"])*100.0, np.nan
                        )
                        comp = comp.rename(columns={"æ—¥ä»˜_only_x":"A_æ—¥ä»˜", "æ—¥ä»˜_only_y":"B_æ—¥ä»˜"})

                        import altair as alt

                        # 1) ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼ˆç›¸å¯¾æ—¥ï¼‰: A/Bã‚’åŒã˜xè»¸ä¸Šã«é‡ã­ã‚‹
                        rel_long = pd.concat([
                            comp[["ç›¸å¯¾æ—¥","A_åˆè¨ˆ"]].rename(columns={"A_åˆè¨ˆ":"å€¤"}).assign(ç³»åˆ—="A"),
                            comp[["ç›¸å¯¾æ—¥","B_åˆè¨ˆ"]].rename(columns={"B_åˆè¨ˆ":"å€¤"}).assign(ç³»åˆ—="B"),
                        ], ignore_index=True)

                        st.markdown("##### æ™‚ç³»åˆ—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï½œç›¸å¯¾æ—¥ï¼‰")
                        line_rel = alt.Chart(rel_long).mark_line().encode(
                            x=alt.X("ç›¸å¯¾æ—¥:Q", title="ç›¸å¯¾æ—¥ï¼ˆå„æœŸé–“ã®é–‹å§‹æ—¥ã‚’1æ—¥ç›®ã¨ã—ã¦æ•´åˆ—ï¼‰"),
                            y=alt.Y("å€¤:Q", title="æ—¥åˆ¥åˆè¨ˆï¼ˆãƒ‡ãƒãƒ³ãƒ‰å€¤ï¼‰"),
                            color="ç³»åˆ—:N",
                            tooltip=["ç³»åˆ—","ç›¸å¯¾æ—¥","å€¤:Q"]
                        ).properties(height=320)
                        st.altair_chart(line_rel, use_container_width=True)

                        # 2) å®Ÿæ—¥ä»˜ã§ã®å‚ç…§ï¼ˆé‡ãªã‚‰ãªã„ãŒå®Ÿã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼åŸºæº–ã§ç¢ºèªï¼‰
                        real_long = pd.concat([
                            Ag.rename(columns={"A_åˆè¨ˆ":"å€¤","æ—¥ä»˜_only":"æ—¥ä»˜"}).assign(ç³»åˆ—="A")[["æ—¥ä»˜","å€¤","ç³»åˆ—"]],
                            Bg.rename(columns={"B_åˆè¨ˆ":"å€¤","æ—¥ä»˜_only":"æ—¥ä»˜"}).assign(ç³»åˆ—="B")[["æ—¥ä»˜","å€¤","ç³»åˆ—"]],
                        ], ignore_index=True)

                        with st.expander("ğŸ—“ å®Ÿæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®æ™‚ç³»åˆ—ï¼ˆå‚è€ƒï¼‰"):
                            line_real = alt.Chart(real_long).mark_line().encode(
                                x=alt.X("æ—¥ä»˜:T", title="æ—¥ä»˜ï¼ˆå®Ÿã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ï¼‰"),
                                y=alt.Y("å€¤:Q", title="æ—¥åˆ¥åˆè¨ˆ"),
                                color="ç³»åˆ—:N",
                                tooltip=["ç³»åˆ—","æ—¥ä»˜:T","å€¤:Q"]
                            ).properties(height=300)
                            st.altair_chart(line_real, use_container_width=True)

                        # æ—¥åˆ¥ã®å·®åˆ†ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯ï¼‰
                        with st.expander("ğŸ§¾ æ—¥åˆ¥ æ¯”è¼ƒè¡¨ï¼ˆBãŒAã‹ã‚‰ã©ã‚Œã ã‘å¤‰åŒ–?ï¼‰", expanded=True):
                            view = comp[["ç›¸å¯¾æ—¥","A_æ—¥ä»˜","B_æ—¥ä»˜","A_åˆè¨ˆ","B_åˆè¨ˆ","å·®åˆ†(Bâˆ’A)","å¢—æ¸›ç‡(%)"]]
                            st.dataframe(view, use_container_width=True)
                            st.download_button(
                                f"{csel}_æ—¥åˆ¥_AvsB_æ¯”è¼ƒ.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=view.to_csv(index=False).encode("utf-8-sig"),
                                file_name=f"{csel}_æ—¥åˆ¥_AvsB_æ¯”è¼ƒ.csv",
                                mime="text/csv",
                            )

            except Exception as e:
                st.error("æ¯”è¼ƒãƒ„ãƒ¼ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                st.exception(e)

# ===== å¯è¦–åŒ–ï¼ˆåŸºæœ¬ãƒ©ã‚¤ãƒ³ï¼‰=====
st.subheader("å¯è¦–åŒ–ï¼ˆç²’åº¦åˆ¥ï¼‰")
import altair as alt
line = alt.Chart(show).mark_line().encode(
    x=alt.X(f"{xcol}:T", title=xcol),
    y=alt.Y("ãƒ‡ãƒãƒ³ãƒ‰å€¤:Q"),
    color="åº—èˆ—å:N",
    tooltip=list(show.columns)
).properties(height=360)
st.altair_chart(line, use_container_width=True)

# ===== è¡¨ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯ï¼‰=====
st.subheader("è¡¨ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯ï¼‰")
st.dataframe(show.reset_index(drop=True), use_container_width=True)
st.download_button(
    "ï¼ˆè¡¨ç¤ºä¸­ã®è¡¨ï¼‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=show.to_csv(index=False).encode("utf-8-sig"),
    file_name="ãƒ‡ãƒãƒ³ãƒ‰å€¤_æŠ½å‡ºçµæœ.csv",
    mime="text/csv",
)

# ===== ãƒ‡ãƒãƒƒã‚° =====
with st.expander("ğŸ§ª ãƒ‡ãƒãƒƒã‚°"):
    st.write({
        "use_fast": use_fast,
        "parquet_available": parquet_available(pq_dir),
        "parquet_dir": str(pq_dir),
        "sidebar_period": (str(start_date), str(end_date)),
        "n_rows_show": len(show),
        "stores_selected": (selected_stores[:10] if selected_stores else []),
    })
